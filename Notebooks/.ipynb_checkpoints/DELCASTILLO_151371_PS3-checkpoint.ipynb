{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3 CS129.18\n",
    "\n",
    "The following problem set will revolve around the Enron Emails dataset.\n",
    "The dataset `data/enron-data/` directory has 6 files. The objective is to build a classifier for whether email is spam or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "\n",
    ">Spam or Ham?\n",
    "\n",
    "Using the lessons on Naive Bayes and TF-IDF, and the other resource notebooks, show the following in this Jupyter Notebook.\n",
    "\n",
    "Write your answers down as Markdown cells or comments in the code.\n",
    "\n",
    "**Using Enron 1 and 2**\n",
    "\n",
    "1. How many Spam Emails are there? ( 1 pt )\n",
    "\n",
    "2. Structure the email data from the 3 directories into 1 dataframe with columns: Status, Subject, Body ( 7 pts )\n",
    "\n",
    "3. Build a Naive Bayes classifier to classify whether emails are spam or not. ( 3 pts )\n",
    "\n",
    "4. What is the longest ham email? ( 1 pt )\n",
    "\n",
    "5. What is the accuracy of your model?( 1 pt )\n",
    "\n",
    "6. Include the Subject in the analysis of the emails, does the accuracy/performance of the model increase? (7 pts)\n",
    "\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "Bonus : Answer questions 1-6 using Enron 1,2, and 3. (5 pts)\n",
    "\n",
    "----\n",
    "\n",
    "**Submit this file on Moodle on the submission link I will provide. This is due October 18 12nn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/enron-data/enron1 ['spam', 'ham'] 2\n",
      "data/enron-data/enron1/spam [] 1500\n",
      "data/enron-data/enron1/ham [] 3672\n"
     ]
    }
   ],
   "source": [
    "rootdirs = [\n",
    "    \"data/enron-data/enron1\",\n",
    "#     \"data/enron-data/enron2\",\n",
    "#     \"data/enron-data/enron3\",\n",
    "#     \"data/enron-data/enron4\",\n",
    "#     \"data/enron-data/enron5\",\n",
    "#     \"data/enron-data/enron6\",\n",
    "]\n",
    "\n",
    "# Loop through all the directories, sub directories and files in the above folder, and print them.\n",
    "for rootdir in rootdirs:\n",
    "    for directories, subdirs, files in os.walk(rootdir):\n",
    "        print(directories, subdirs, len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3672\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "ham_list = []\n",
    "spam_list = []\n",
    "\n",
    "# Same as before, but this time, read the files, and append them to the ham and spam list\n",
    "for rootdir in rootdirs:\n",
    "    for directories, subdirs, files in os.walk(rootdir):\n",
    "        if (os.path.split(directories)[1]  == 'ham'):\n",
    "            for filename in files:      \n",
    "                with open(os.path.join(directories, filename), encoding=\"latin-1\") as f:\n",
    "                    data = f.read()\n",
    "                    ham_list.append(data)\n",
    "\n",
    "        if (os.path.split(directories)[1]  == 'spam'):\n",
    "            for filename in files:\n",
    "                with open(os.path.join(directories, filename), encoding=\"latin-1\") as f:\n",
    "                    data = f.read()\n",
    "                    spam_list.append(data)\n",
    "\n",
    "\n",
    "print(len(ham_list))\n",
    "print(len(spam_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1500 spam emails\n"
     ]
    }
   ],
   "source": [
    "# 1. Number of Spam Emails\n",
    "print('There are {spam} spam emails'.format(spam=len(spam_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>raw</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>ena sales on hpl</td>\n",
       "      <td>just to update you on this project   s status ...</td>\n",
       "      <td>ena sales on hpl just to update you on this pr...</td>\n",
       "      <td>Subject: ena sales on hpl\\njust to update you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>98   6736   98   9638 for 1997   ua 4 issues</td>\n",
       "      <td>the above referenced meters need to be placed ...</td>\n",
       "      <td>98   6736   98   9638 for 1997   ua 4 issues  ...</td>\n",
       "      <td>Subject: 98 - 6736 &amp; 98 - 9638 for 1997 ( ua 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>hpl nominations for december 28   1999</td>\n",
       "      <td>see attached file   hpll 228   xls     hpll ...</td>\n",
       "      <td>hpl nominations for december 28   1999   see a...</td>\n",
       "      <td>Subject: hpl nominations for december 28 , 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>revised nom   kcs resources</td>\n",
       "      <td>daren   it   s in   bob                       ...</td>\n",
       "      <td>revised nom   kcs resources daren   it   s in ...</td>\n",
       "      <td>Subject: revised nom - kcs resources\\ndaren ,\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>new production   sitara deals needed</td>\n",
       "      <td>daren   fyi   bob                             ...</td>\n",
       "      <td>new production   sitara deals needed daren   f...</td>\n",
       "      <td>Subject: new production - sitara deals needed\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  status                                         subject  \\\n",
       "0    ham                                ena sales on hpl   \n",
       "1    ham  98   6736   98   9638 for 1997   ua 4 issues     \n",
       "2    ham          hpl nominations for december 28   1999   \n",
       "3    ham                     revised nom   kcs resources   \n",
       "4    ham            new production   sitara deals needed   \n",
       "\n",
       "                                                body  \\\n",
       "0  just to update you on this project   s status ...   \n",
       "1  the above referenced meters need to be placed ...   \n",
       "2    see attached file   hpll 228   xls     hpll ...   \n",
       "3  daren   it   s in   bob                       ...   \n",
       "4  daren   fyi   bob                             ...   \n",
       "\n",
       "                                                 raw  \\\n",
       "0  ena sales on hpl just to update you on this pr...   \n",
       "1  98   6736   98   9638 for 1997   ua 4 issues  ...   \n",
       "2  hpl nominations for december 28   1999   see a...   \n",
       "3  revised nom   kcs resources daren   it   s in ...   \n",
       "4  new production   sitara deals needed daren   f...   \n",
       "\n",
       "                                            original  \n",
       "0  Subject: ena sales on hpl\\njust to update you ...  \n",
       "1  Subject: 98 - 6736 & 98 - 9638 for 1997 ( ua 4...  \n",
       "2  Subject: hpl nominations for december 28 , 199...  \n",
       "3  Subject: revised nom - kcs resources\\ndaren ,\\...  \n",
       "4  Subject: new production - sitara deals needed\\...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Structure Data\n",
    "data = {\n",
    "    'status': [],\n",
    "    'subject': [],\n",
    "    'body': [],\n",
    "    'raw': [],\n",
    "    'original': [],\n",
    "}\n",
    "\n",
    "import re\n",
    "\n",
    "def mike_clean(string):\n",
    "    return re.sub(r\"[^a-zA-Z0-9]\", r\" \", string.strip())\n",
    "\n",
    "def mike_append(status, the_list):\n",
    "    for string in the_list:\n",
    "        # removes Subject: fw: or re:\n",
    "        subject = mike_clean(re.sub(r\"^Subject:(.{0,4}\\:){0,1}\", r\"\", string.split('\\n', 1)[0]))\n",
    "        body = mike_clean(''.join(string.split('\\n', 1)[1:]))\n",
    "        data['status'].append(status)\n",
    "        data['subject'].append(subject)\n",
    "        data['body'].append(body)\n",
    "        data['raw'].append(subject + \" \" + body)\n",
    "        data['original'].append(string)\n",
    "\n",
    "mike_append(\"ham\", ham_list)\n",
    "mike_append(\"spam\", spam_list)\n",
    "# mike_append(\"test\", spam_list)\n",
    "\n",
    "df = pd.DataFrame(data=data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5172"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "vectorize\n",
      "indexed\n",
      "classifier\n",
      "(1707, 26314)\n",
      "Accuracy: 93.09 percent\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(data[\"body\"], data[\"status\"], test_size=0.33, random_state=2574)\n",
    "print(\"split\")\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_train_x = vectorizer.fit_transform(train_x)\n",
    "print(\"vectorize\")\n",
    "\n",
    "index_value={i[1]:i[0] for i in vectorizer.vocabulary_.items()}\n",
    "fully_indexed = []\n",
    "for row in tfidf_train_x:\n",
    "    fully_indexed.append({index_value[column]:value for (column,value) in zip(row.indices,row.data)})\n",
    "print(\"indexed\")\n",
    "\n",
    "# 3. NB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(tfidf_train_x.toarray(), train_y)\n",
    "print(\"classifier\")\n",
    "\n",
    "#5. Accuracy\n",
    "\n",
    "tfidf_test_x = vectorizer.fit_transform(test_x)\n",
    "print(tfidf_test_x.shape)\n",
    "scores = cross_val_score(classifier, tfidf_test_x.toarray(), test_y, cv=5)\n",
    "acc = scores.mean()\n",
    "print(\"Accuracy: %0.2f percent\" % (acc *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest ham email is 31860 characters long.\n"
     ]
    }
   ],
   "source": [
    "# 4. Longest Ham Email\n",
    "\n",
    "ham_list.sort(key=len)\n",
    "longest_email = ham_list[len(ham_list) - 1]\n",
    "\n",
    "print('The longest ham email is {len} characters long.'.format(len=len(longest_email),email=longest_email))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. With Subject\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(data[\"raw\"], data[\"status\"])\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_train_x = vectorizer.fit_transform(train_x)\n",
    "\n",
    "index_value={i[1]:i[0] for i in vectorizer.vocabulary_.items()}\n",
    "fully_indexed = []\n",
    "for row in tfidf_train_x:\n",
    "    fully_indexed.append({index_value[column]:value for (column,value) in zip(row.indices,row.data)})\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(tfidf_train_x.toarray(), train_y)\n",
    "\n",
    "tfidf_test_x = vectorizer.transform(test_x)\n",
    "print(tfidf_test_x.shape)\n",
    "scores = cross_val_score(classifier, tfidf_test_x.toarray(), test_y, cv=5)\n",
    "acc = scores.mean()\n",
    "print(\"Accuracy: %0.2f percent\" % (acc *100))\n",
    "\n",
    "# Answer: Parsing the raw body including the subject gives \n",
    "# the model more text to work with. While the improvement \n",
    "# may be little, there certainly is one. Adding the subject \n",
    "# improves its accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,26314) (38727,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-eb08be667a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Please gas up my car'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You won the lottery, claim it now!!!\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' == '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"\"\"\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0mjointi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_prior_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mn_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m             n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) /\n\u001b[0m\u001b[1;32m    434\u001b[0m                                  (self.sigma_[i, :]), 1)\n\u001b[1;32m    435\u001b[0m             \u001b[0mjoint_log_likelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjointi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_ij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,26314) (38727,) "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
